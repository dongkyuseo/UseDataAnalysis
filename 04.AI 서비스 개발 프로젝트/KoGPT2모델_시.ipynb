{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_path = 'D:/workspace/03.UseDataAnalysis/04.AI 서비스 개발 프로젝트/gpt/gptdata/melon_ballad.csv'\n",
        "model_save = 'D:/workspace/03.UseDataAnalysis/04.AI 서비스 개발 프로젝트/gpt/gptdata/8.발라드가사.csv'\n",
        "output_path = 'gpt/result/발라드가사'\n",
        "input =  '제주 애월읍 해변위에 앉아 사진을 찍는다.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cleasing(text):\n",
        "    pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)' # E-mail주소제거\n",
        "    text = re.sub(pattern=pattern, repl='', string=text)\n",
        "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+' # URL제거\n",
        "    text = re.sub(pattern=pattern, repl='', string=text)\n",
        "    pattern = '([ㄱ-ㅎㅏ-ㅣ]+)' # 한글 자음, 모음 제거\n",
        "    text = re.sub(pattern=pattern, repl='', string=text)\n",
        "    pattern = '<[^>]*>'         # HTML 태그 제거\n",
        "    text = re.sub(pattern=pattern, repl='', string=text)\n",
        "    text = re.sub(pattern='\\n[0-9]', repl='', string=text)\n",
        "    # pattern = '[^\\w\\s]' # 특수기호제거\n",
        "    # text = re.sub(pattern=pattern, repl='', string=text)\n",
        "    return text    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>contents</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>울고 있는 그 여잘 아나요그댈 기다리는 바보 같은 사람멀어져 간단 걸 알면서도그댈 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>그 언젠가 별빛에 새긴 추억 하나가바람에 두고 온 너의 이름을 불러와너의 이름은 떠...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>언제였죠 그대 내게 처음 오던 날아직도 난 설레는걸요눈이 부시던 그대 모습내 맘 흔...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>어디까지 말을 할까어떡하면 맘을 알까기다릴 수도 더 버틸 수도 없는무심히 나를 보네...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>언젠가는 그대를 잊을 때도 오겠지오랜 시간 지나면애써 그댈 지우려 하지는 않아도 되...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>오늘일줄야 늘생각은 했지만이른거아냐 좀서운하다네가 했던 따뜻한 말의 온도여기 그대론...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>집으로 가는 밤길꿈도 잠이 든 이 밤가로등 빛 아래공허하게 날아드는 하루살이몸을 누...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>내일이 되면널 사랑하게 될 거야예정된 일인 걸불안한 화음들과부딪는 현의 떨림점점 선...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>You called me your loverYou called me your fri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>하얀꽃 내리는 새벽길 모퉁이어두운 눈길에 언손 비비며차가운 신문지 끈묶어 감아쥐고달...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              contents\n",
              "0    울고 있는 그 여잘 아나요그댈 기다리는 바보 같은 사람멀어져 간단 걸 알면서도그댈 ...\n",
              "1    그 언젠가 별빛에 새긴 추억 하나가바람에 두고 온 너의 이름을 불러와너의 이름은 떠...\n",
              "2    언제였죠 그대 내게 처음 오던 날아직도 난 설레는걸요눈이 부시던 그대 모습내 맘 흔...\n",
              "3    어디까지 말을 할까어떡하면 맘을 알까기다릴 수도 더 버틸 수도 없는무심히 나를 보네...\n",
              "4    언젠가는 그대를 잊을 때도 오겠지오랜 시간 지나면애써 그댈 지우려 하지는 않아도 되...\n",
              "..                                                 ...\n",
              "995  오늘일줄야 늘생각은 했지만이른거아냐 좀서운하다네가 했던 따뜻한 말의 온도여기 그대론...\n",
              "996  집으로 가는 밤길꿈도 잠이 든 이 밤가로등 빛 아래공허하게 날아드는 하루살이몸을 누...\n",
              "997  내일이 되면널 사랑하게 될 거야예정된 일인 걸불안한 화음들과부딪는 현의 떨림점점 선...\n",
              "998  You called me your loverYou called me your fri...\n",
              "999  하얀꽃 내리는 새벽길 모퉁이어두운 눈길에 언손 비비며차가운 신문지 끈묶어 감아쥐고달...\n",
              "\n",
              "[1000 rows x 1 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "df = pd.read_csv(data_path)\n",
        "df.columns = ['contents']\n",
        "df = df[['contents']]\n",
        "df[\"contents\"] = df[\"contents\"].str.replace(pat=r'[^a-zA-Z0-9ㄱ-ㅣ가-힣. ]', repl=r'', regex=True)\n",
        "df[\"contents\"] = df[\"contents\"].str.replace(pat=r'[\\n]', repl=r'', regex=True)\n",
        "df_new = []\n",
        "tmp = df['contents']\n",
        "for i in range(len(tmp)):\n",
        "    tmp2 = str(tmp[i]).split('.  ')\n",
        "    tmp2 = list(filter(None, tmp2))\n",
        "    tmp2 = [item + '.' for item in tmp2]\n",
        "    df_new.extend(tmp2)\n",
        "df_new = pd.DataFrame(df_new, columns=['contents'])\n",
        "df_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 9787.20it/s]\n"
          ]
        }
      ],
      "source": [
        "import tqdm\n",
        "for i in tqdm.tqdm(range(len(df_new))):\n",
        "    df_new['contents'][i] = cleasing(df_new['contents'][i])\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_new.to_csv(model_save)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 모델링"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bPSvUgTT4aWH"
      },
      "outputs": [],
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import GPT2LMHeadModel\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "def load_dataset(file_path, tokenizer, block_size = 128):\n",
        "    dataset = TextDataset(\n",
        "        tokenizer = tokenizer,\n",
        "        file_path = file_path,\n",
        "        block_size = block_size,\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "def load_data_collator(tokenizer, mlm = False):\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer = tokenizer,\n",
        "        mlm = mlm,\n",
        "    )\n",
        "    return data_collator\n",
        "\n",
        "def train(train_file_path, model_name, output_dir, overwrite_output_dir,\n",
        "          per_device_train_batch_size, num_train_epochs, save_steps):\n",
        "    tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name,\n",
        "                bos_token = '</s>', eos_token = '</s>', unk_token = '<unk>',\n",
        "                pad_token = '<pad>', mask_token = '<mask>')\n",
        "    train_dataset = load_dataset(train_file_path, tokenizer)\n",
        "    data_collator = load_data_collator(tokenizer)\n",
        "\n",
        "    tokenizer.save_pretrained(output_dir, legacy_format = False)\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    model.save_pretrained(output_dir)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir = output_dir,\n",
        "        overwrite_output_dir = overwrite_output_dir,\n",
        "        per_device_eval_batch_size = per_device_train_batch_size,\n",
        "        num_train_epochs = num_train_epochs,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model = model,\n",
        "        args = training_args,\n",
        "        data_collator = data_collator,\n",
        "        train_dataset = train_dataset,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DvpLKULL5BGH"
      },
      "outputs": [],
      "source": [
        "model_name = 'skt/kogpt2-base-v2'\n",
        "output_dir = output_path\n",
        "overwrite_output_dir = False\n",
        "per_device_train_batch_size = 8\n",
        "num_train_epochs = 5.0\n",
        "save_steps = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_file_path = model_save\n",
        "train(train_file_path,\n",
        "    model_name = model_name,\n",
        "    output_dir = output_dir,\n",
        "    overwrite_output_dir = overwrite_output_dir,\n",
        "    per_device_train_batch_size = per_device_train_batch_size,\n",
        "    num_train_epochs = num_train_epochs,\n",
        "    save_steps = save_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LvSkmCk5REU"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
        "\n",
        "def load_model(model_path):\n",
        "  model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "  return model\n",
        "\n",
        "def load_tokenizer(tokenizer_path):\n",
        "  tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)\n",
        "  return tokenizer\n",
        "\n",
        "def generate_text(sequence, max_lenth):\n",
        "  model_path =output_path\n",
        "  model = load_model(model_path)\n",
        "  tokenizer = load_tokenizer(model_path)\n",
        "  ids = tokenizer.encode(f'{sequence},', return_tensors = 'pt')\n",
        "  final_outputs = model.generate(\n",
        "      ids,\n",
        "      do_sample = True,\n",
        "      max_length = max_length,\n",
        "      pad_token_id = model.config.pad_token_id,\n",
        "      repetition_penalty = 1.5, # 단어 빈도 패널티\n",
        "      no_repeat_ngram_size=2, # 반복 줄이기 n개 이상의 토큰이 반복될 경우 등장확률을 0으로 만듬\n",
        "      temperature=0.9, # 모델의 다음 토큰 확률분포에 변형을 줘 문장을 다양하게 생성\n",
        "      tok_k = 50, # 가장 높은 확률을 지닌 n개의 단어수 중 추출\n",
        "      top_p = 0.9 # 누적확률이 n%인 단어까지 포함하여 추출\n",
        "  )\n",
        "  result = tokenizer.decode(final_outputs[0], skip_special_tokens = True)\n",
        "  result = re.sub(pattern='\\n[0-9][0-9][0-9][0-9],', repl='', string=result)\n",
        "  result = re.sub(pattern='\\n[0-9][0-9][0-9],', repl='', string=result)\n",
        "  result = re.sub(pattern='  ', repl=' ', string=result)\n",
        "  result = re.sub(pattern=',', repl=' ', string=result)\n",
        "  result = re.sub(pattern='  ', repl=' ', string=result)\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJK5mZ3C5Veg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file result/브런치_임재건시_3000\\config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file result/브런치_임재건시_3000\\pytorch_model.bin\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input :제주 애월읍 해변위에 앉아 사진을 찍었다.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at result/브런치_임재건시_3000.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file result/브런치_임재건시_3000\\added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file result/브런치_임재건시_3000\\special_tokens_map.json\n",
            "loading file result/브런치_임재건시_3000\\tokenizer_config.json\n",
            "loading file result/브런치_임재건시_3000\\tokenizer.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'제주 애월읍 해변위에 앉아 사진을 찍었다. 사진 찍는 것도 일이라지만 저기서는 제 집도 못 찾은 어린 숨의 집을 보여 주고 싶었다.당신은 나를 꽃처럼 피우시어 당신이 나보다 고운 까닭으로 비에 시리다 지다가 구름 사이 띄운 하얀 잎 붉게 물든 봄날 좋은 날 좋다 하시었네.5월의 여름은 푸르렀어도 한겨울 눈 위로 붉었을까요.아무렴 어때. 푸른 하늘 나도 붉은 빛이야.7월에 오신다는 손님 어른 아이 이름 다 불러줘서 고맙'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input = '제주 애월읍 해변위에 앉아 사진을 찍었다.'\n",
        "sequence = input\n",
        "max_length = 128 \n",
        "print('input :' + sequence)\n",
        "result = generate_text(sequence, max_length)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiX8t-NEtvcs"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file result/브런치_임재건시_3000\\config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file result/브런치_임재건시_3000\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at result/브런치_임재건시_3000.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file result/브런치_임재건시_3000\\added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file result/브런치_임재건시_3000\\special_tokens_map.json\n",
            "loading file result/브런치_임재건시_3000\\tokenizer_config.json\n",
            "loading file result/브런치_임재건시_3000\\tokenizer.json\n",
            "loading configuration file result/브런치_임재건시_3000\\config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file result/브런치_임재건시_3000\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at result/브런치_임재건시_3000.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file result/브런치_임재건시_3000\\added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file result/브런치_임재건시_3000\\special_tokens_map.json\n",
            "loading file result/브런치_임재건시_3000\\tokenizer_config.json\n",
            "loading file result/브런치_임재건시_3000\\tokenizer.json\n",
            "loading configuration file result/브런치_임재건시_3000\\config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file result/브런치_임재건시_3000\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at result/브런치_임재건시_3000.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file result/브런치_임재건시_3000\\added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file result/브런치_임재건시_3000\\special_tokens_map.json\n",
            "loading file result/브런치_임재건시_3000\\tokenizer_config.json\n",
            "loading file result/브런치_임재건시_3000\\tokenizer.json\n",
            "loading configuration file result/브런치_임재건시_3000\\config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file result/브런치_임재건시_3000\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at result/브런치_임재건시_3000.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file result/브런치_임재건시_3000\\added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file result/브런치_임재건시_3000\\special_tokens_map.json\n",
            "loading file result/브런치_임재건시_3000\\tokenizer_config.json\n",
            "loading file result/브런치_임재건시_3000\\tokenizer.json\n",
            "loading configuration file result/브런치_임재건시_3000\\config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file result/브런치_임재건시_3000\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at result/브런치_임재건시_3000.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file result/브런치_임재건시_3000\\added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file result/브런치_임재건시_3000\\special_tokens_map.json\n",
            "loading file result/브런치_임재건시_3000\\tokenizer_config.json\n",
            "loading file result/브런치_임재건시_3000\\tokenizer.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['는 것도 일이라지만 저기서는 제 집도 못 찾은 어린 숨의 집을 보여 주고 싶었다.',\n",
              " '세상에 없는 그런 집은 여럿일 수 없어.',\n",
              " '다만 나는 그렇게 있잖아.',\n",
              " '러니까 너도 네 집인 줄 아느냐 묻지 마.',\n",
              " '내 사랑 잘 알아서 모두 내 탓이야.']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_result = []\n",
        "for _ in range(5):\n",
        "    tmp1 = result.strip(input)\n",
        "    tmp1 = tmp1.strip(', ')\n",
        "    tmp1 = tmp1.strip(',. ')\n",
        "    tmp2 = tmp1.split('.')\n",
        "    input = tmp2[0]+'.'\n",
        "    input = input.replace('\\n', '')\n",
        "    max_length = 128 \n",
        "    output_result.append(input)\n",
        "    globals()['result'] = generate_text(input, max_length)\n",
        "output_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file result/브런치_임재건시_3000\\config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file result/브런치_임재건시_3000\\pytorch_model.bin\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input :제주 애월읍 해변위에 앉아 사진을 찍었다. 행복함.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at result/브런치_임재건시_3000.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file result/브런치_임재건시_3000\\added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file result/브런치_임재건시_3000\\special_tokens_map.json\n",
            "loading file result/브런치_임재건시_3000\\tokenizer_config.json\n",
            "loading file result/브런치_임재건시_3000\\tokenizer.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'제주 애월읍 해변위에 앉아 사진을 찍었다. 행복함. 우리가 행복한 줄도 알아.오래 참회가 희망인가. 다시 한 번 소회를 물으니 나는 그게 무죄다 하는 편이 낫다.5분만이라 하여도 못 들은 척 하고 일어나듯 세심하게 살핀 후 다음 기회에 넘긴다.내놓는 글이 너 하나뿐인 아침이 반갑다고 읽히면 좋겠어.나는 지난번 담장에 꽃밭이던 시절에 본 것을 시작으로 모두 오래된 것이라며 새로 본다.꽃처럼 피었다가 잎대로 벌건 낯 붉히네'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input = '제주 애월읍 해변위에 앉아 사진을 찍었다. 행복함.'\n",
        "sequence = input\n",
        "max_length = 128 \n",
        "print('input :' + sequence)\n",
        "result = generate_text(sequence, max_length).strip(r'[^a-zA-Z0-9ㄱ-ㅣ가-힣. ]')\n",
        "result = re.sub(pattern='\\n[0-9][0-9][0-9][0-9],', repl='', string=result)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file result/브런치_임재건시_3000\\config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file result/브런치_임재건시_3000\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at result/브런치_임재건시_3000.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file result/브런치_임재건시_3000\\added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file result/브런치_임재건시_3000\\special_tokens_map.json\n",
            "loading file result/브런치_임재건시_3000\\tokenizer_config.json\n",
            "loading file result/브런치_임재건시_3000\\tokenizer.json\n",
            "loading configuration file result/브런치_임재건시_3000\\config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file result/브런치_임재건시_3000\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at result/브런치_임재건시_3000.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file result/브런치_임재건시_3000\\added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file result/브런치_임재건시_3000\\special_tokens_map.json\n",
            "loading file result/브런치_임재건시_3000\\tokenizer_config.json\n",
            "loading file result/브런치_임재건시_3000\\tokenizer.json\n",
            "loading configuration file result/브런치_임재건시_3000\\config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file result/브런치_임재건시_3000\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at result/브런치_임재건시_3000.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file result/브런치_임재건시_3000\\added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file result/브런치_임재건시_3000\\special_tokens_map.json\n",
            "loading file result/브런치_임재건시_3000\\tokenizer_config.json\n",
            "loading file result/브런치_임재건시_3000\\tokenizer.json\n",
            "loading configuration file result/브런치_임재건시_3000\\config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file result/브런치_임재건시_3000\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at result/브런치_임재건시_3000.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file result/브런치_임재건시_3000\\added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file result/브런치_임재건시_3000\\special_tokens_map.json\n",
            "loading file result/브런치_임재건시_3000\\tokenizer_config.json\n",
            "loading file result/브런치_임재건시_3000\\tokenizer.json\n",
            "loading configuration file result/브런치_임재건시_3000\\config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file result/브런치_임재건시_3000\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at result/브런치_임재건시_3000.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file result/브런치_임재건시_3000\\added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file result/브런치_임재건시_3000\\special_tokens_map.json\n",
            "loading file result/브런치_임재건시_3000\\tokenizer_config.json\n",
            "loading file result/브런치_임재건시_3000\\tokenizer.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['우리가 행복한 줄도 알아.',\n",
              " '나는 달을 본 적 없는 사람인데 내가 아는 달은 다른 별에 비할까 작은 별이 많았으나 나는 불행하지 않았다.',\n",
              " '니 다행이었다 할 수 있는 것이 아니었어.',\n",
              " '살아보고 또 살아서 얼마든 행복할 것 같았어.',\n",
              " '5월의 봄은 어떤 꽃을 피우려고 초승달인가 보다.']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_result = []\n",
        "for i in range(5):\n",
        "    tmp1 = result.strip(input)\n",
        "    tmp1 = tmp1.strip(',')\n",
        "    tmp1 = tmp1.strip(', ')\n",
        "    tmp1 = tmp1.strip(',. ')\n",
        "    tmp2 = tmp1.split('.')\n",
        "    input = tmp2[0]+'.'\n",
        "    input = input.replace('\\n', '')\n",
        "    max_length = 128 \n",
        "    output_result.append(input)\n",
        "    globals()['result'] = generate_text(input, max_length)\n",
        "output_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "name": "KoGPT2모델(여행+).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
