{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9N2NBlsadu1"
      },
      "outputs": [],
      "source": [
        "data_path = 'D:/workspace/gpt/gptdata/melon_ballad.csv'\n",
        "model_save = 'D:/workspace/gpt/gptdata/8.발라드가사.csv'\n",
        "output_path = 'result/발라드가사'\n",
        "input =  '제주 애월읍 해변위에 앉아 사진을 찍는다.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwaFDTbaadu4"
      },
      "source": [
        "# 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRo3mixFadu5"
      },
      "outputs": [],
      "source": [
        "def cleasing(text):\n",
        "    pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)' # E-mail주소제거\n",
        "    text = re.sub(pattern=pattern, repl='', string=text)\n",
        "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+' # URL제거\n",
        "    text = re.sub(pattern=pattern, repl='', string=text)\n",
        "    pattern = '([ㄱ-ㅎㅏ-ㅣ]+)' # 한글 자음, 모음 제거\n",
        "    text = re.sub(pattern=pattern, repl='', string=text)\n",
        "    pattern = '<[^>]*>'         # HTML 태그 제거\n",
        "    text = re.sub(pattern=pattern, repl='', string=text)\n",
        "    text = re.sub(pattern='\\n[0-9]', repl='', string=text)\n",
        "    # pattern = '[^\\w\\s]' # 특수기호제거\n",
        "    # text = re.sub(pattern=pattern, repl='', string=text)\n",
        "    return text    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpXY-E-kadu6",
        "outputId": "a9d6f8e6-9b75-4453-da27-95354e9c3d85"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>contents</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>울고 있는 그 여잘 아나요그댈 기다리는 바보 같은 사람멀어져 간단 걸 알면서도그댈 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>그 언젠가 별빛에 새긴 추억 하나가바람에 두고 온 너의 이름을 불러와너의 이름은 떠...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>언제였죠 그대 내게 처음 오던 날아직도 난 설레는걸요눈이 부시던 그대 모습내 맘 흔...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>어디까지 말을 할까어떡하면 맘을 알까기다릴 수도 더 버틸 수도 없는무심히 나를 보네...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>언젠가는 그대를 잊을 때도 오겠지오랜 시간 지나면애써 그댈 지우려 하지는 않아도 되...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>오늘일줄야 늘생각은 했지만이른거아냐 좀서운하다네가 했던 따뜻한 말의 온도여기 그대론...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>집으로 가는 밤길꿈도 잠이 든 이 밤가로등 빛 아래공허하게 날아드는 하루살이몸을 누...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>내일이 되면널 사랑하게 될 거야예정된 일인 걸불안한 화음들과부딪는 현의 떨림점점 선...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>You called me your loverYou called me your fri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>하얀꽃 내리는 새벽길 모퉁이어두운 눈길에 언손 비비며차가운 신문지 끈묶어 감아쥐고달...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              contents\n",
              "0    울고 있는 그 여잘 아나요그댈 기다리는 바보 같은 사람멀어져 간단 걸 알면서도그댈 ...\n",
              "1    그 언젠가 별빛에 새긴 추억 하나가바람에 두고 온 너의 이름을 불러와너의 이름은 떠...\n",
              "2    언제였죠 그대 내게 처음 오던 날아직도 난 설레는걸요눈이 부시던 그대 모습내 맘 흔...\n",
              "3    어디까지 말을 할까어떡하면 맘을 알까기다릴 수도 더 버틸 수도 없는무심히 나를 보네...\n",
              "4    언젠가는 그대를 잊을 때도 오겠지오랜 시간 지나면애써 그댈 지우려 하지는 않아도 되...\n",
              "..                                                 ...\n",
              "995  오늘일줄야 늘생각은 했지만이른거아냐 좀서운하다네가 했던 따뜻한 말의 온도여기 그대론...\n",
              "996  집으로 가는 밤길꿈도 잠이 든 이 밤가로등 빛 아래공허하게 날아드는 하루살이몸을 누...\n",
              "997  내일이 되면널 사랑하게 될 거야예정된 일인 걸불안한 화음들과부딪는 현의 떨림점점 선...\n",
              "998  You called me your loverYou called me your fri...\n",
              "999  하얀꽃 내리는 새벽길 모퉁이어두운 눈길에 언손 비비며차가운 신문지 끈묶어 감아쥐고달...\n",
              "\n",
              "[1000 rows x 1 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "df = pd.read_csv(data_path)\n",
        "df.columns = ['contents']\n",
        "df = df[['contents']]\n",
        "df[\"contents\"] = df[\"contents\"].str.replace(pat=r'[^a-zA-Z0-9ㄱ-ㅣ가-힣. ]', repl=r'', regex=True)\n",
        "df[\"contents\"] = df[\"contents\"].str.replace(pat=r'[\\n]', repl=r'', regex=True)\n",
        "df_new = []\n",
        "tmp = df['contents']\n",
        "for i in range(len(tmp)):\n",
        "    tmp2 = str(tmp[i]).split('.  ')\n",
        "    tmp2 = list(filter(None, tmp2))\n",
        "    tmp2 = [item + '.' for item in tmp2]\n",
        "    df_new.extend(tmp2)\n",
        "df_new = pd.DataFrame(df_new, columns=['contents'])\n",
        "df_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IYZdGmaadu7",
        "outputId": "d656ae00-8148-479b-e5fa-40e1d16bb089"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 20888.59it/s]\n"
          ]
        }
      ],
      "source": [
        "import tqdm\n",
        "for i in tqdm.tqdm(range(len(df_new))):\n",
        "    df_new['contents'][i] = cleasing(df_new['contents'][i])\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PW4yAvAadu7"
      },
      "outputs": [],
      "source": [
        "df_new.to_csv(model_save)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTIQWhvvadu8"
      },
      "source": [
        "# 모델링"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPSvUgTT4aWH"
      },
      "outputs": [],
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import GPT2LMHeadModel\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "def load_dataset(file_path, tokenizer, block_size = 128):\n",
        "    dataset = TextDataset(\n",
        "        tokenizer = tokenizer,\n",
        "        file_path = file_path,\n",
        "        block_size = block_size,\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "def load_data_collator(tokenizer, mlm = False):\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer = tokenizer,\n",
        "        mlm = mlm,\n",
        "    )\n",
        "    return data_collator\n",
        "\n",
        "def train(train_file_path, model_name, output_dir, overwrite_output_dir,\n",
        "          per_device_train_batch_size, num_train_epochs, save_steps):\n",
        "    tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name,\n",
        "                bos_token = '</s>', eos_token = '</s>', unk_token = '<unk>',\n",
        "                pad_token = '<pad>', mask_token = '<mask>')\n",
        "    train_dataset = load_dataset(train_file_path, tokenizer)\n",
        "    data_collator = load_data_collator(tokenizer)\n",
        "\n",
        "    tokenizer.save_pretrained(output_dir, legacy_format = False)\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    model.save_pretrained(output_dir)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir = output_dir,\n",
        "        overwrite_output_dir = overwrite_output_dir,\n",
        "        per_device_eval_batch_size = per_device_train_batch_size,\n",
        "        num_train_epochs = num_train_epochs,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model = model,\n",
        "        args = training_args,\n",
        "        data_collator = data_collator,\n",
        "        train_dataset = train_dataset,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvpLKULL5BGH"
      },
      "outputs": [],
      "source": [
        "model_name = 'skt/kogpt2-base-v2'\n",
        "output_dir = output_path\n",
        "overwrite_output_dir = False\n",
        "per_device_train_batch_size = 8\n",
        "num_train_epochs = 5.0\n",
        "save_steps = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbe7YigHadu-",
        "outputId": "afa020e9-ebf3-4673-8c78-09ee46b25ec0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: 100%|██████████| 2.69M/2.69M [00:01<00:00, 2.12MB/s]\n",
            "Downloading: 100%|██████████| 0.98k/0.98k [00:00<00:00, 995kB/s]\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
            "C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n",
            "Downloading: 100%|██████████| 490M/490M [00:06<00:00, 85.1MB/s]\n",
            "***** Running training *****\n",
            "  Num examples = 1749\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1095\n",
            " 46%|████▌     | 500/1095 [00:54<01:00,  9.84it/s]Saving model checkpoint to result/발라드가사\\checkpoint-500\n",
            "Configuration saved in result/발라드가사\\checkpoint-500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.5264, 'learning_rate': 2.71689497716895e-05, 'epoch': 2.28}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in result/발라드가사\\checkpoint-500\\pytorch_model.bin\n",
            " 91%|█████████▏| 1000/1095 [01:49<00:09,  9.82it/s]Saving model checkpoint to result/발라드가사\\checkpoint-1000\n",
            "Configuration saved in result/발라드가사\\checkpoint-1000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.5968, 'learning_rate': 4.337899543378996e-06, 'epoch': 4.57}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in result/발라드가사\\checkpoint-1000\\pytorch_model.bin\n",
            "100%|█████████▉| 1094/1095 [02:03<00:00,  9.84it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "100%|██████████| 1095/1095 [02:04<00:00,  8.82it/s]\n",
            "Saving model checkpoint to result/발라드가사\n",
            "Configuration saved in result/발라드가사\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 124.3228, 'train_samples_per_second': 70.341, 'train_steps_per_second': 8.808, 'train_loss': 2.9972877397929154, 'epoch': 5.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in result/발라드가사\\pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "train_file_path = model_save\n",
        "train(train_file_path,\n",
        "    model_name = model_name,\n",
        "    output_dir = output_dir,\n",
        "    overwrite_output_dir = overwrite_output_dir,\n",
        "    per_device_train_batch_size = per_device_train_batch_size,\n",
        "    num_train_epochs = num_train_epochs,\n",
        "    save_steps = save_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LvSkmCk5REU"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
        "\n",
        "def load_model(model_path):\n",
        "  model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "  return model\n",
        "\n",
        "def load_tokenizer(tokenizer_path):\n",
        "  tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)\n",
        "  return tokenizer\n",
        "\n",
        "def generate_text(sequence, max_lenth):\n",
        "  model_path =output_path\n",
        "  model = load_model(model_path)\n",
        "  tokenizer = load_tokenizer(model_path)\n",
        "  ids = tokenizer.encode(f'{sequence},', return_tensors = 'pt')\n",
        "  final_outputs = model.generate(\n",
        "      ids,\n",
        "      do_sample = True,\n",
        "      max_length = max_length,\n",
        "      pad_token_id = model.config.pad_token_id,\n",
        "      repetition_penalty = 1.5, # 단어 빈도 패널티\n",
        "      no_repeat_ngram_size=2, # 반복 줄이기 n개 이상의 토큰이 반복될 경우 등장확률을 0으로 만듬\n",
        "      temperature=0.9, # 모델의 다음 토큰 확률분포에 변형을 줘 문장을 다양하게 생성\n",
        "      tok_k = 50, # 가장 높은 확률을 지닌 n개의 단어수 중 추출\n",
        "      top_p = 0.9 # 누적확률이 n%인 단어까지 포함하여 추출\n",
        "  )\n",
        "  result = tokenizer.decode(final_outputs[0], skip_special_tokens = True)\n",
        "  result = re.sub(pattern='\\n[0-9][0-9][0-9][0-9],', repl='', string=result)\n",
        "  result = re.sub(pattern='\\n[0-9][0-9][0-9],', repl='', string=result)\n",
        "  result = re.sub(pattern='  ', repl=' ', string=result)\n",
        "  result = re.sub(pattern=',', repl=' ', string=result)\n",
        "  result = re.sub(pattern='  ', repl=' ', string=result)\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJK5mZ3C5Veg",
        "outputId": "9df4a9a6-3995-45ca-9d83-d1b7e856c9ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file result/발라드가사\\config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file result/발라드가사\\pytorch_model.bin\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input :제주 애월읍 해변위에 앉아 사진을 찍었다.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at result/발라드가사.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file result/발라드가사\\added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file result/발라드가사\\special_tokens_map.json\n",
            "loading file result/발라드가사\\tokenizer_config.json\n",
            "loading file result/발라드가사\\tokenizer.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'제주 애월읍 해변위에 앉아 사진을 찍었다. Dont have to goAnd I swear appearing downWhenever you know we hady forget우리가 마주한 이 순간들이어느새 빛나버렸죠 그댄 섬처럼 따뜻해요Yeah Were makes not alright그대는 행복합니다만 우리가 어떻게 우린영원할 수 있습니까.7월의 어느날문득 네 생각이 났었지내가 있는 곳 여기 너가 있어매일같이 날아다니는 수많은 밤'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input = '제주 애월읍 해변위에 앉아 사진을 찍었다.'\n",
        "sequence = input\n",
        "max_length = 128 \n",
        "print('input :' + sequence)\n",
        "result = generate_text(sequence, max_length)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiX8t-NEtvcs",
        "outputId": "773eb9ca-87ec-4078-a9d8-215d2b5242dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file result/발라드가사\\config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file result/발라드가사\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at result/발라드가사.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file result/발라드가사\\added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file result/발라드가사\\special_tokens_map.json\n",
            "loading file result/발라드가사\\tokenizer_config.json\n",
            "loading file result/발라드가사\\tokenizer.json\n",
            "loading configuration file result/발라드가사\\config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file result/발라드가사\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at result/발라드가사.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file result/발라드가사\\added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file result/발라드가사\\special_tokens_map.json\n",
            "loading file result/발라드가사\\tokenizer_config.json\n",
            "loading file result/발라드가사\\tokenizer.json\n",
            "loading configuration file result/발라드가사\\config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file result/발라드가사\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at result/발라드가사.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file result/발라드가사\\added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file result/발라드가사\\special_tokens_map.json\n",
            "loading file result/발라드가사\\tokenizer_config.json\n",
            "loading file result/발라드가사\\tokenizer.json\n",
            "loading configuration file result/발라드가사\\config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file result/발라드가사\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at result/발라드가사.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file result/발라드가사\\added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file result/발라드가사\\special_tokens_map.json\n",
            "loading file result/발라드가사\\tokenizer_config.json\n",
            "loading file result/발라드가사\\tokenizer.json\n",
            "loading configuration file result/발라드가사\\config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file result/발라드가사\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at result/발라드가사.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file result/발라드가사\\added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file result/발라드가사\\special_tokens_map.json\n",
            "loading file result/발라드가사\\tokenizer_config.json\n",
            "loading file result/발라드가사\\tokenizer.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Dont have to goAnd I swear appearing downWhenever you know we hady forget우리가 마주한 이 순간들이어느새 빛나버렸죠 그댄 섬처럼 따뜻해요Yeah Were makes not alright그대는 행복합니다만 우리가 어떻게 우린영원할 수 있습니까.',\n",
              " 'ce of loveEven thats into please tell but debtforryHoldings c.',\n",
              " 'Theres not say youI want to haveMy hand and I knowYour everydrast momentBeautizen the darkAnd fineWere gonna learching foreverspecial onCaused without changelisperish from airSo HighThe view at anciistD.',\n",
              " 'bluebreakthings insideL.',\n",
              " 'Im still highThe past is come darkness the worldHere my lifeAnd I special everyday you and i kill findYeah that apart of nightBut gonna tell our ratherWas only foreverEven just stop reachingMemories tooAfterJust alive AwardNobody Originality attempto.']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_result = []\n",
        "for _ in range(5):\n",
        "    tmp1 = result.strip(input)\n",
        "    tmp1 = tmp1.strip(', ')\n",
        "    tmp1 = tmp1.strip(',. ')\n",
        "    tmp2 = tmp1.split('.')\n",
        "    input = tmp2[0]+'.'\n",
        "    input = input.replace('\\n', '')\n",
        "    max_length = 128 \n",
        "    output_result.append(input)\n",
        "    globals()['result'] = generate_text(input, max_length)\n",
        "output_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkz3MMAnadvA",
        "outputId": "4dce613b-4e37-4faa-bf25-dc7cf618bfb3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file result/발라드가사\\config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file result/발라드가사\\pytorch_model.bin\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input :제주 애월읍 해변위에 앉아 사진을 찍었다. 행복함.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at result/발라드가사.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file result/발라드가사\\added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file result/발라드가사\\special_tokens_map.json\n",
            "loading file result/발라드가사\\tokenizer_config.json\n",
            "loading file result/발라드가사\\tokenizer.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'제주 애월읍 해변위에 앉아 사진을 찍었다. 행복함. 수많은 사진 속에 비친 나의 맘그대 모습 보여줘서미소를 지어도 잊지 말아 줬으면 해.오랜만이야 우린 잊어갔던 우리서로가 서로를 알지 못했던마음의 상처가 이제 너무 깊어진 것 같아나 혼자 남겨진 시간들다 흘려보낸 것들이내게만 남아 더 미웠고너와 함께한 추억이 다 어디로 사라졌는지혼자 남은 기억들로하루 하루 지나보내지도 못한 나였지그래 너 없이 살아갈 수 없어우리가 서로 모를 리 없지니가 떠난 뒤엔 내 사랑들도 전부'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input = '제주 애월읍 해변위에 앉아 사진을 찍었다. 행복함.'\n",
        "sequence = input\n",
        "max_length = 128 \n",
        "print('input :' + sequence)\n",
        "result = generate_text(sequence, max_length).strip(r'[^a-zA-Z0-9ㄱ-ㅣ가-힣. ]')\n",
        "result = re.sub(pattern='\\n[0-9][0-9][0-9][0-9],', repl='', string=result)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDBZfE3radvA",
        "outputId": "360e04a2-4f7e-420e-85e0-9ad5a9778c56"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file result/발라드가사\\config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file result/발라드가사\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at result/발라드가사.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file result/발라드가사\\added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file result/발라드가사\\special_tokens_map.json\n",
            "loading file result/발라드가사\\tokenizer_config.json\n",
            "loading file result/발라드가사\\tokenizer.json\n",
            "loading configuration file result/발라드가사\\config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file result/발라드가사\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at result/발라드가사.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file result/발라드가사\\added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file result/발라드가사\\special_tokens_map.json\n",
            "loading file result/발라드가사\\tokenizer_config.json\n",
            "loading file result/발라드가사\\tokenizer.json\n",
            "loading configuration file result/발라드가사\\config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file result/발라드가사\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at result/발라드가사.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file result/발라드가사\\added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file result/발라드가사\\special_tokens_map.json\n",
            "loading file result/발라드가사\\tokenizer_config.json\n",
            "loading file result/발라드가사\\tokenizer.json\n",
            "loading configuration file result/발라드가사\\config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file result/발라드가사\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at result/발라드가사.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file result/발라드가사\\added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file result/발라드가사\\special_tokens_map.json\n",
            "loading file result/발라드가사\\tokenizer_config.json\n",
            "loading file result/발라드가사\\tokenizer.json\n",
            "loading configuration file result/발라드가사\\config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file result/발라드가사\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at result/발라드가사.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file result/발라드가사\\added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file result/발라드가사\\special_tokens_map.json\n",
            "loading file result/발라드가사\\tokenizer_config.json\n",
            "loading file result/발라드가사\\tokenizer.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['수많은 사진 속에 비친 나의 맘그대 모습 보여줘서미소를 지어도 잊지 말아 줬으면 해.',\n",
              " '118 7월의 어느날 우연히네게 전화를 걸었는데속삭인다이젠 안녕 전화기 너머로 들려오는너의 목소리는아직도 내 귓가에 생생해너와 함께했던 그때에 난어둠 속을 헤매고 있어오늘 밤만 기다려왔는데이대로 잠들면 어떨까내일 아침에 또 연락올꺼야왜 이런지 몰라답한 마음에 계속 네 연락을 기다리곤 했는지또다시 생각이 나서계속 생각해보니내가 이럴수가 있나 봐끝나지 않는 긴밤.',\n",
              " '주치지 않기를그대의 손길도 아직은 서툴렀던 것 같아미안하다는 말.',\n",
              " '이 밤 난 너의 곁을 떠나보네.',\n",
              " '그 마음처럼모든 게 꿈만 같아좋았던 시간들이 다시 돌아와그땐 참 좋았었는데어디를 둘러봐도 나 혼자만의 착각 속에오늘따라 더 보고 싶어너와 걷고 싶은 그런 나라서네게 기대 어젠밤새 눈물이 흘러끝이 없다는 걸 알아아쉬움에 빠져 또 하루를 보내야 해내일이면 다 끝나겠지늦은 출발이라 후회할 거야나에게도 미련으로 남길 바래잊으려고 애써 볼수록더 아쉬워지는 널 보며혼자 눈물짓던 나를다시는 잡지 말자고하루.']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_result = []\n",
        "for i in range(5):\n",
        "    tmp1 = result.strip(input)\n",
        "    tmp1 = tmp1.strip(',')\n",
        "    tmp1 = tmp1.strip(', ')\n",
        "    tmp1 = tmp1.strip(',. ')\n",
        "    tmp2 = tmp1.split('.')\n",
        "    input = tmp2[0]+'.'\n",
        "    input = input.replace('\\n', '')\n",
        "    max_length = 128 \n",
        "    output_result.append(input)\n",
        "    globals()['result'] = generate_text(input, max_length)\n",
        "output_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0_bRMphadvA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "name": "KoGPT2모델_발라드가사.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
