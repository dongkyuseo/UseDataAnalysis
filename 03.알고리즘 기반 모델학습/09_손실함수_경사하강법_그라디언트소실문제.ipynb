{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 경사하강법\n",
    "- 기울기 소실, 그라디언트 소실 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형회귀 뉴런\n",
    "\n",
    "class Neuron:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.w = 1.0     # 가중치를 초기화합니다\n",
    "        self.b = 1.0     # 절편을 초기화합니다\n",
    "    \n",
    "    def forpass(self, x):\n",
    "        y_hat = x * self.w + self.b       # 직선 방정식을 계산합니다\n",
    "        return y_hat\n",
    "    \n",
    "    def backprop(self, x, err):\n",
    "        w_grad = x * err    # 가중치에 대한 그래디언트를 계산합니다\n",
    "        b_grad = 1 * err    # 절편에 대한 그래디언트를 계산합니다\n",
    "        return w_grad, b_grad\n",
    "\n",
    "    def fit(self, x, y, epochs=100):\n",
    "        for i in range(epochs):           # 에포크만큼 반복합니다\n",
    "            for x_i, y_i in zip(x, y):    # 모든 샘플에 대해 반복합니다, 배치 1 자료임\n",
    "                y_hat = self.forpass(x_i) # 정방향 계산\n",
    "\n",
    "                err = -(y_i - y_hat)      # 오차 계산\n",
    "\n",
    "                w_grad, b_grad = self.backprop(x_i, err)  # 역방향 계산\n",
    "                self.w -= w_grad          # 가중치 업데이트\n",
    "                self.b -= b_grad          # 절편 업데이트\n",
    "            print(self.w, self.b)\n",
    "            print(y_i, y_hat)\n",
    "            print('-------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.044490607004028e+18 -1.726732813004091e+18\n",
      "1 1.9539285067530368e+18\n",
      "-------------------------------\n",
      "-1.062089077986025e+36 -6.023812512584651e+35\n",
      "1 6.816398518076288e+35\n",
      "-------------------------------\n",
      "-3.7051623906544444e+53 -2.101443657843168e+53\n",
      "1 2.3779421097880142e+53\n",
      "-------------------------------\n",
      "-1.292568450769889e+71 -7.33101410089283e+70\n",
      "1 8.295595779072652e+70\n",
      "-------------------------------\n",
      "-4.5092037103144836e+88 -2.5574688879667512e+88\n",
      "1 2.8939690771489294e+88\n",
      "-------------------------------\n",
      "-1.573063158783047e+106 -8.921885871316658e+105\n",
      "1 1.00957872617444e+106\n",
      "-------------------------------\n",
      "-5.487726571013394e+123 -3.1124541876278203e+123\n",
      "1 3.521976832413652e+123\n",
      "-------------------------------\n",
      "-1.9144268143374556e+141 -1.0857985867344807e+141\n",
      "1 1.228663053852348e+141\n",
      "-------------------------------\n",
      "-6.678594459886605e+158 -3.787874454959117e+158\n",
      "1 4.286265843683082e+158\n",
      "-------------------------------\n",
      "-2.3298683253694645e+176 -1.3214230578143556e+176\n",
      "1 1.4952899271383212e+176\n",
      "-------------------------------\n",
      "-8.127887456205985e+193 -4.609864763171745e+193\n",
      "1 5.216409918895929e+193\n",
      "-------------------------------\n",
      "-2.8354630079910012e+211 -1.6081793797273045e+211\n",
      "1 1.8197763489273268e+211\n",
      "-------------------------------\n",
      "-9.891685278622569e+228 -5.6102316450616996e+228\n",
      "1 6.348400550576709e+228\n",
      "-------------------------------\n",
      "-3.450774613372385e+246 -1.9571634550238305e+246\n",
      "1 2.21467817044215e+246\n",
      "-------------------------------\n",
      "-1.2038237263805781e+264 -6.827683832008133e+263\n",
      "1 7.726039589904924e+263\n",
      "-------------------------------\n",
      "-4.199612337997789e+281 -2.3818790602391287e+281\n",
      "1 2.6952759340586767e+281\n",
      "-------------------------------\n",
      "-1.4650603242793662e+299 -8.309330070336619e+298\n",
      "1 9.402634138980986e+298\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n",
      "nan nan\n",
      "1 nan\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-de5e106e0728>:14: RuntimeWarning: overflow encountered in multiply\n",
      "  w_grad = x * err    # 가중치에 대한 그래디언트를 계산합니다\n",
      "<ipython-input-22-de5e106e0728>:26: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  self.w -= w_grad          # 가중치 업데이트\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x=np.array([20,10,20,10,100,20,4,2])\n",
    "y=np.array([200,5,100,20,30,40,5,1])\n",
    "\n",
    "neuron = Neuron()\n",
    "neuron.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------예측값\n",
      "[ 21  11  21  11 101  21   5   3]\n",
      "\n",
      "-------err: 예측값-실제값\n",
      "[-179    6  -79   -9   71  -19    0    2]\n",
      "\n",
      "-------w갱신, b갱신\n",
      "[ 3581   -59  1581    91 -7099   381     1    -3] [180  -5  80  10 -70  20   1  -1]\n"
     ]
    }
   ],
   "source": [
    "w=1;b=1\n",
    "y_hat = x * w + b       # 직선 방정식을 계산합니다\n",
    "print('\\n-------예측값')\n",
    "print(y_hat) #정방향 계산\n",
    "err = -(y - y_hat)\n",
    "\n",
    "print('\\n-------err: 예측값-실제값')\n",
    "print(err)\n",
    "\n",
    "w_grad = x * err    # 가중치에 대한 그래디언트를 계산합니다\n",
    "b_grad = 1 * err\n",
    "\n",
    "w -= w_grad\n",
    "b -= b_grad\n",
    "\n",
    "\n",
    "print('\\n-------w갱신, b갱신')\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 찾아서 공부할 것\n",
    "\n",
    "1. 그라디언트 소실 gradient vanishing : 기울기가 너무 작아짐\n",
    "2. 그라디언트 폭주 gradient exploding : 기울기가 너무 커짐\n",
    "3. 그라디언트 클리핑 gradient clipping : 최적화 함수 사용시 그라디언트 임계값을 설정함\n",
    "4. 배치 정규화 : 배치사이즈를 받아서 그 자료를 정규화(평균을 0으로 만듬)\n",
    "\n",
    "https://codetorial.net/tensorflow/basics_of_optimizer.html"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
